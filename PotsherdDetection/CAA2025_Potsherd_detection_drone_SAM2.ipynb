{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juergenlandauer/caa2025/blob/main/PotsherdDetection/CAA2025_Potsherd_detection_drone_SAM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wkV75Db9tXF"
      },
      "source": [
        "## Automatic potsherd detection in videos with SAM (Segment Anything Model by Meta AI)\n",
        "Author: Juergen Landauer (juergen AT landauer-ai.de)\n",
        "\n",
        "To start, first go to the \"Input parameters\" section below and review or (optionally) adjust parameters. Then run the entire Notebook by choosing Runtime->Run all in the menu above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo5LAKqyNzfW"
      },
      "source": [
        "### Install SAM2 and dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first install Meta's SAM, Roboflow's Supervision and other libraries, then the SAM model weights. This could take up to 5 minutes."
      ],
      "metadata": {
        "id": "qBxmA2Byrwwd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaIKHk2mlVuo"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq opencv-python supervision\n",
        "!pip install -Uq 'git+https://github.com/facebookresearch/sam2.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caution!\n",
        "You might get asked to restart this session here. This is mandatory."
      ],
      "metadata": {
        "id": "vRJAuAwQMpvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download SAM weights\n",
        "!mkdir -p ../checkpoints/\n",
        "#!wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt\n",
        "!wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt"
      ],
      "metadata": {
        "id": "tUblg7yLFVC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNkBmgfhpg_c"
      },
      "source": [
        "# Input parameters\n",
        "\n",
        "Review all parameters in this section and (optionally) adjust them on the right side. For example, you can upload your own input zip file by providing an URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez7ezScuXoyb"
      },
      "outputs": [],
      "source": [
        "# demo video frames for CAA2 025\n",
        "# feel free to replace this with your own imagery by providing a download URL (e.g. from Google Drive)\n",
        "# Note that the ZIP file must contain only images in its root folder. No sub-directories!\n",
        "\n",
        "INPUT_ZIP_URL = 'https://www.dropbox.com/scl/fi/u0htcr0kllnohu7hwsfox/DroneVideo.zip?rlkey=zxj171l892fgogi4mo8h5ej8e&st=33q8o1ka&dl=0' # @param {\"allow-input\":true}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Note:** If you run your file(s) for the first time, you might want to play with the hue and area range parameters below. This is because your particular type(s) of pottery may have deviating color (hue) values. This process is called calibration.\n",
        "\n",
        "Only one or a few video frames (images) are sufficient for that process.\n",
        "\n",
        "To make this easier, the software has a calibration mode, which you can turn on or off here. It will output your image(s) also with hue values for each detection in a separate ZIP file.\n",
        "\n",
        "Later you might want to turn it off for performance reasons and run your entire video."
      ],
      "metadata": {
        "id": "e-G2tQypSh9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Calibration_Mode = \"True\" # @param [\"True\", \"False\"]"
      ],
      "metadata": {
        "id": "y4g72idNaiSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hue_from = 10 # @param {\"allow-input\":true}\n",
        "hue_to = 17   # @param {\"allow-input\":true}\n",
        "HUE_RANGE = (hue_from, hue_to)"
      ],
      "metadata": {
        "id": "hZd-PKe6NR4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AREA_RANGE = (0, 500)"
      ],
      "metadata": {
        "id": "BBB113SEQzwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGdef_3Spg_d"
      },
      "outputs": [],
      "source": [
        "# do you want output whileprocessing (here in the Colab console)? Recommended\n",
        "With_console_output = True # @param [\"True\", \"False\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do you want the number of sherds per image written on the output images?\n",
        "Show_number_sherds_on_output = True# @param [\"True\", \"False\"]"
      ],
      "metadata": {
        "id": "Jxit_qNWQ9rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHRLQPV4WKd1"
      },
      "source": [
        "### Now we import some libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIcNq3IiXufS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import base64\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files as colabfiles\n",
        "\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svThmVIGZZAc"
      },
      "source": [
        "**NOTE:** This code enables mixed-precision computing for faster deep learning. It uses bfloat16 for most calculations and, on newer NVIDIA GPUs, leverages TensorFloat-32 (TF32) for certain operations to further boost performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZAXCVT0ZWkd"
      },
      "outputs": [],
      "source": [
        "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "if torch.cuda.get_device_properties(0).major >= 8:\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLeXwS2UQU5y"
      },
      "source": [
        "## Let's load the model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHvgsf08QRZo"
      },
      "outputs": [],
      "source": [
        "# We use the small SAM model. Do not forget to edit here if you want to use another one!\n",
        "DEVICE = torch.device('cuda')\n",
        "CHECKPOINT = \"../checkpoints/sam2.1_hiera_small.pt\"\n",
        "CONFIG = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
        "\n",
        "sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We load the data (video frames or orthophoto) and unzip it into the directory 'input'"
      ],
      "metadata": {
        "id": "jFwAP-16FDIp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3bSYkcHs9AE"
      },
      "outputs": [],
      "source": [
        "!rm -rf input output file.zip\n",
        "!mkdir -p input/\n",
        "!wget -O file.zip \"$INPUT_ZIP_URL\"\n",
        "!unzip file.zip -d input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyl2Ldc_Wvtm"
      },
      "source": [
        "### Results visualisation helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHQ4h-2HlVut"
      },
      "outputs": [],
      "source": [
        "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX, color=sv.Color.RED)\n",
        "\n",
        "def annotateMask(image_bgr, detections):\n",
        "    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTSzflTWlVuu"
      },
      "outputs": [],
      "source": [
        "round_box_annotator = sv.RoundBoxAnnotator(color_lookup=sv.ColorLookup.INDEX, color=sv.Color.RED)\n",
        "\n",
        "def annotateBox(image_bgr, detections):\n",
        "    annotated_image = round_box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps6mMNJflVuu"
      },
      "outputs": [],
      "source": [
        "box_corner_annotator = sv.BoxCornerAnnotator(color_lookup=sv.ColorLookup.INDEX, color=sv.Color.RED, thickness=1, corner_length=6)\n",
        "\n",
        "def annotateBoxCorner(image_bgr, detections):\n",
        "    annotated_image = box_corner_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hJXzOIblVuu"
      },
      "outputs": [],
      "source": [
        "dot_annotator = sv.DotAnnotator(color_lookup=sv.ColorLookup.INDEX, color=sv.Color.RED, radius=2)\n",
        "\n",
        "def annotateDot(image_bgr, detections):\n",
        "    annotated_image = dot_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
        "    return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER, color_lookup=sv.ColorLookup.INDEX, color=sv.Color.RED)\n",
        "\n",
        "def annotateHue(image_bgr, detections):\n",
        "    annotated_image = label_annotator.annotate(scene=image_bgr.copy(), detections=detections,\n",
        "                                               labels=list(map(str, detections['hue'])))\n",
        "    return annotated_image"
      ],
      "metadata": {
        "id": "2vQM6VkysFu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If desired you can implement a filter function here in order to filter your input images. This is optional"
      ],
      "metadata": {
        "id": "V5pMHVu4OU7C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p-Pm-_nlVuv"
      },
      "outputs": [],
      "source": [
        "def preprocessSnippetNONE(image: np.ndarray): return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNJh3faHlVuv"
      },
      "outputs": [],
      "source": [
        "filter = preprocessSnippetNONE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for reading images and processing"
      ],
      "metadata": {
        "id": "MYq3oPCpeFCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = str(pow(2,40)) # for very large images"
      ],
      "metadata": {
        "id": "vQtYOcpYVdRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_tiler(image_path, tile_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Splits an image into tiles of a specified size, yielding them one by one.\n",
        "    Pads tiles smaller than tile_size with black pixels.\n",
        "    \"\"\"\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Could not read image at {image_path}\")\n",
        "        return\n",
        "\n",
        "    img_h, img_w = img.shape[:2]\n",
        "    tile_h, tile_w = tile_size\n",
        "\n",
        "    for y in range(0, img_h, tile_h):\n",
        "        for x in range(0, img_w, tile_w):\n",
        "            tile = img[y:min(y + tile_h, img_h), x:min(x + tile_w, img_w)]\n",
        "\n",
        "            # Pad the tile if necessary\n",
        "            padded_tile = np.zeros((tile_h, tile_w, 3), dtype=np.uint8)\n",
        "            tile_h_pad, tile_w_pad = tile.shape[:2]\n",
        "\n",
        "            padded_tile[:tile_h_pad, :tile_w_pad, :] = tile\n",
        "\n",
        "            yield img_w, img_h, x, y, tile.shape, padded_tile"
      ],
      "metadata": {
        "id": "PDAe8M0MUnRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mean_color_with_mask_hsv(image, mask):\n",
        "    \"\"\"Computes the mean color of a HSV image within a given mask.\n",
        "\n",
        "    Args:\n",
        "        image: The input image as a NumPy array (BGR format).\n",
        "        mask: The binary mask as a NumPy array.\n",
        "\n",
        "    Returns:\n",
        "        A tuple representing the mean BGR color (mean_b, mean_g, mean_r).\n",
        "        Returns None if the mask is empty.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the mask and image have the same shape\n",
        "    if image.shape[:2] != mask.shape: raise ValueError(\"Image and mask dimensions must match\")\n",
        "\n",
        "    # Get the indices where the mask is non-zero\n",
        "    mask_indices = np.where(mask != 0)\n",
        "\n",
        "    if not mask_indices[0].size:  # Check if mask is empty\n",
        "      return None\n",
        "\n",
        "    # Extract the HSV color values at the masked indices\n",
        "    masked_h = image[mask_indices][:, 0]\n",
        "    masked_s = image[mask_indices][:, 1]\n",
        "    masked_v = image[mask_indices][:, 2]\n",
        "\n",
        "    # Calculate the mean color values\n",
        "    mean_h = np.mean(masked_h).astype(np.uint8)\n",
        "    mean_s = np.mean(masked_s).astype(np.uint8)\n",
        "    mean_v = np.mean(masked_v).astype(np.uint8)\n",
        "\n",
        "    return (mean_h, mean_s, mean_v)"
      ],
      "metadata": {
        "id": "T4T6Z0gVecbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering SAM results\n",
        "based on area and color (hue), then applying Non-Maximum Suppression (NMS) to avoid overlapping results."
      ],
      "metadata": {
        "id": "VhopR8vyOq1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBHcXn2TlVuv"
      },
      "outputs": [],
      "source": [
        "# returns a filtered sam2 result, optionally stores snippets as a side effect\n",
        "def filter_result(img, sam2_result, hue_range=(10,17)):\n",
        "    # prepare\n",
        "    hsv_img   = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "\n",
        "    # put into dataframe\n",
        "    result_df = pd.DataFrame(sam2_result)\n",
        "    print (\"before all filters:\", len(result_df))\n",
        "\n",
        "    # filter by area\n",
        "    print (\"--> filter areas (min max) = (\", result_df.area.min(), result_df.area.max(), result_df.area.mean(), \") / below \", AREA_RANGE[1],\":\", len(result_df[result_df.area < AREA_RANGE[1]]))\n",
        "    result_df = result_df[AREA_RANGE[0] <= result_df.area]\n",
        "    result_df = result_df[result_df.area <= AREA_RANGE[1]]\n",
        "    print (\"after area filter:\", len(result_df))\n",
        "\n",
        "    # filter by  HUE\n",
        "    h_from, h_to = hue_range\n",
        "    result_df['hue'] = -1\n",
        "    result_df['sat'] = -1\n",
        "    for index, row in result_df.iterrows():\n",
        "        hue, sat, val = compute_mean_color_with_mask_hsv(hsv_img, row.segmentation)\n",
        "        result_df.at[index, 'hue'] = hue\n",
        "        result_df.at[index, 'sat'] = sat\n",
        "\n",
        "    # if failed hue test -> drop them\n",
        "    result_df = result_df[(result_df.hue >= h_from) & (result_df.hue <= h_to)]\n",
        "    print (\"remaining after hue filter\", len(result_df))\n",
        "\n",
        "    if (len(result_df) > 0):\n",
        "        # convert into sv.Detections and apply NMS\n",
        "        xyxy       = sv.xywh_to_xyxy(xywh=np.array(result_df['bbox'].values.tolist(), dtype=np.uint16))\n",
        "        confidence = np.array(result_df['predicted_iou'].values.tolist())\n",
        "        masks      = np.array(result_df['segmentation'].values.tolist()) # masks are too slow, use boxes instead\n",
        "        detections = sv.Detections(xyxy=xyxy, data={'items': masks,\n",
        "                                                    'hue': list(result_df.hue),\n",
        "                                                    'sat': list(result_df.sat),\n",
        "                                                    'bbox': list(result_df.bbox),\n",
        "                                                    }, confidence=confidence)\n",
        "        print (\"before NMS\", len(detections))\n",
        "        detections = detections.with_nms(class_agnostic=True)\n",
        "        detections.mask = detections.data['items'] # re-insert the masks\n",
        "        print (\"remaining after NMS\", len(detections))\n",
        "    else: detections = []\n",
        "\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOWsfs1BlVuv"
      },
      "source": [
        "## process folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLrVcMK-lVuw"
      },
      "outputs": [],
      "source": [
        "OUTPATH = Path(\"./output\")\n",
        "os.makedirs(OUTPATH, exist_ok=True)\n",
        "\n",
        "files = sorted(glob('input/*.*'))\n",
        "len(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnknYHvvlVuw"
      },
      "outputs": [],
      "source": [
        "# limit files list to some files only for debugging if desired\n",
        "#files = files[60:80]\n",
        "files = files[100:110]\n",
        "#files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM-Z8opJlVux"
      },
      "outputs": [],
      "source": [
        "mask_generator = SAM2AutomaticMaskGenerator(\n",
        "    model=sam2_model,\n",
        "    points_per_side=64, # this controls the quality of the output. Increase this value if output quality is insufficient (if you do not get out-of-memory errors through this)-\n",
        "    points_per_batch=96,\n",
        "    pred_iou_thresh=0.7,\n",
        "    stability_score_thresh=0.92,\n",
        "    stability_score_offset=0.7,\n",
        "    crop_n_layers=1,\n",
        "    box_nms_thresh=0.7,\n",
        "    #crop_n_points_downscale_factor=2,\n",
        "    #min_mask_region_area=15,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choose other annotation graphics if desired by uncommenting below\n",
        "# Recommended:\n",
        "# Mask: shows outline of potsherds\n",
        "# Box = shows bounding box of potsherds\n",
        "\n",
        "annotatorFunc = annotateMask\n",
        "#annotatorFunc = annotateBox\n",
        "#annotatorFunc = annotateBoxCorner\n",
        "#annotatorFunc = annotateDot"
      ],
      "metadata": {
        "id": "qx_zW3r7rnzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G5PW8xflVuy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# this is the main loop\n",
        "# we iterate over all files, then over all tiles of each file if required\n",
        "\n",
        "for f in files:\n",
        "    filepath = Path(f)\n",
        "    FIRST_TIME = True\n",
        "    total = 0 # total no. of detections in this file\n",
        "    for tile_width, tile_height, x, y, orig_shape, tile in image_tiler(filepath):\n",
        "        print ('_________________')\n",
        "        print ( filepath, tile_width, tile_height, x, y)\n",
        "        if FIRST_TIME: FIRST_TIME = False; annot_out = np.zeros((tile_height, tile_width, 3), dtype=np.uint8)\n",
        "        image_bgr = tile\n",
        "        if image_bgr.max() == 0: continue # skip empty tiles\n",
        "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "        sam2_result = mask_generator.generate(image_rgb)\n",
        "        torch.cuda.empty_cache();gc.collect()\n",
        "\n",
        "        # now we filter the results\n",
        "        detections = filter_result(image_rgb, sam2_result, hue_range=HUE_RANGE)\n",
        "        if detections:\n",
        "            annot_rgb = annotatorFunc(image_rgb, detections)\n",
        "            if Calibration_Mode: # add hue values\n",
        "                for d in range(len(detections)):\n",
        "                  cv2.putText(annot_rgb, str(detections.data['hue'][d]), tuple(map(int, detections.data['bbox'][d][:2])), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0))\n",
        "        else:\n",
        "            annot_rgb = image_rgb # if nothing found\n",
        "        if With_console_output: sv.plot_images_grid(images=[image_bgr, cv2.cvtColor(annot_rgb, cv2.COLOR_RGB2BGR)], grid_size=(1, 2), titles=['source image', 'segmented image'])\n",
        "        # write result onto image\n",
        "        annot_rgb = annot_rgb[0:orig_shape[0], 0:orig_shape[1]] # go back to original shape (remove padding)\n",
        "        annot_out[y:y+tile.shape[0], x:x+tile.shape[1]] = annot_rgb\n",
        "        total += len(detections)\n",
        "        cv2.imwrite(OUTPATH/(\"SAM_\"+filepath.name),cv2.cvtColor(annot_out, cv2.COLOR_RGB2BGR))\n",
        "        #break\n",
        "\n",
        "    print (\"total number of sherds: #\", total)\n",
        "    if Show_number_sherds_on_output:\n",
        "        cv2.putText(annot_out, \"# \"+str(total), (40,120), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 255, 0), 3, cv2.LINE_AA)\n",
        "        cv2.imwrite(OUTPATH/(\"SAM_\"+filepath.name),cv2.cvtColor(annot_out, cv2.COLOR_RGB2BGR))\n",
        "    #break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export results for download\n",
        "We now open a file download dialog for the output.zip. Simply store the output in your local computer. Done :-)"
      ],
      "metadata": {
        "id": "VczJI9S3ATLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gvtcAD-mrW7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tniXtbDg32TM"
      },
      "outputs": [],
      "source": [
        "!zip -r output.zip output\n",
        "colabfiles.download('output.zip')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H29xi9Fg5YYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}